{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ELIFsAXQXoxi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "M82OtXKdXxKP",
        "outputId": "79752fd6-f6d4-4b6c-ae85-eeced7345477"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad20a511-9c08-4cfc-b8ed-162422f270b4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad20a511-9c08-4cfc-b8ed-162422f270b4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Pride and Prejudice.txt to Pride and Prejudice (1).txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eaH-NrGoYE6k"
      },
      "outputs": [],
      "source": [
        "### load and preprocess file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "6TGB2HBaX4xX",
        "outputId": "22aaf5aa-c112-4f98-b244-979e8a3bdf25"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Project Gutenberg eBook of Pride and Prejudice, by Jane AustenThis eBook is for the use of anyone anywhere in the United States andmost other parts of the world at no cost and with almost no restrictionswhatsoever. You may copy it, give it away or re-use it under the termsof the Project Gutenberg License included with this eBook or online atwww.gutenberg.org. If you are not located in the United States, youwill have to check the laws of the country where you are located beforeusing this eBoo'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file = open(\"Pride and Prejudice.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# Store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "  lines.append(i)\n",
        "\n",
        "#converting list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ''.join(lines)\n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')\n",
        "\n",
        "#remove unnecessary spaces \n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBdW5NCcYBpT",
        "outputId": "04c8469e-08af-465d-cee8-55265a91e8ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 223, 186, 912, 3, 328, 4, 1351, 30, 72, 4174, 912, 23, 21, 1]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZFNvtpQYitL",
        "outputId": "31cd693f-7aca-4441-a3ee-76da9f86c04e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "125076"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequence_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO2NzzZ9Yq_c",
        "outputId": "6fbf950e-19b4-4993-e535-f90d0522a51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7200\n"
          ]
        }
      ],
      "source": [
        "#vocab_size= unique words\n",
        "vocab_size = len(tokenizer.word_index) + 1    # not 0= bcz reserved for padding\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaYj-QFRYvO2",
        "outputId": "3ce24468-ea98-4446-f20c-2b462fb562a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Length of sequences are:  125073\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[   1,  223,  186,  912],\n",
              "       [ 223,  186,  912,    3],\n",
              "       [ 186,  912,    3,  328],\n",
              "       [ 912,    3,  328,    4],\n",
              "       [   3,  328,    4, 1351],\n",
              "       [ 328,    4, 1351,   30],\n",
              "       [   4, 1351,   30,   72],\n",
              "       [1351,   30,   72, 4174],\n",
              "       [  30,   72, 4174,  912],\n",
              "       [  72, 4174,  912,   23]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]        #3 words to predict next word\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6lXu6OxNZOkJ"
      },
      "outputs": [],
      "source": [
        "#output has = 3 columns from input and 1 column is for output "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NUPn0oQ-ZfyZ"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIQ73cguZld3",
        "outputId": "96586a5e-997f-4b45-ecd6-edeba74b6091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data:  [[   1  223  186]\n",
            " [ 223  186  912]\n",
            " [ 186  912    3]\n",
            " [ 912    3  328]\n",
            " [   3  328    4]\n",
            " [ 328    4 1351]\n",
            " [   4 1351   30]\n",
            " [1351   30   72]\n",
            " [  30   72 4174]\n",
            " [  72 4174  912]]\n",
            "Response:  [ 912    3  328    4 1351   30   72 4174  912   23]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyIgEOV1aNFZ",
        "outputId": "d7ea25dc-1e47-45b5-c36a-12f59865a93b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)   #binary class metrics\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKSMAMH0atEX"
      },
      "source": [
        "**CREATING THE** **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "u-fEEvjkcvGI"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))  #softmax=no: to probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7kBnqEdcvjo",
        "outputId": "2163b75b-3b49-46e9-8267-d5daffc77d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 3, 10)             72000     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 7200)              7207200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,328,200\n",
            "Trainable params: 20,328,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLMBiO_Nc_BC"
      },
      "source": [
        "**Plot the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "IpzwF5ebdCoJ",
        "outputId": "a7b8283e-7d28-4e39-a32e-30e8ca74c828"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtP5xJZ7dHGv"
      },
      "source": [
        "**Build model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8jcW-pkdJn_",
        "outputId": "82af611d-4fb9-4105-e556-ef09dec365b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 6.2218\n",
            "Epoch 00001: loss improved from inf to 6.22180, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 39s 15ms/step - loss: 6.2218\n",
            "Epoch 2/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 5.5947\n",
            "Epoch 00002: loss improved from 6.22180 to 5.59464, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 5.5946\n",
            "Epoch 3/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 5.2881\n",
            "Epoch 00003: loss improved from 5.59464 to 5.28806, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 31s 16ms/step - loss: 5.2881\n",
            "Epoch 4/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 5.0676\n",
            "Epoch 00004: loss improved from 5.28806 to 5.06739, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 5.0674\n",
            "Epoch 5/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 4.8714\n",
            "Epoch 00005: loss improved from 5.06739 to 4.87172, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 4.8717\n",
            "Epoch 6/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 4.6772\n",
            "Epoch 00006: loss improved from 4.87172 to 4.67712, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 4.6771\n",
            "Epoch 7/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 4.4772\n",
            "Epoch 00007: loss improved from 4.67712 to 4.47720, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 4.4772\n",
            "Epoch 8/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 4.2758\n",
            "Epoch 00008: loss improved from 4.47720 to 4.27577, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 32s 16ms/step - loss: 4.2758\n",
            "Epoch 9/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 4.0665\n",
            "Epoch 00009: loss improved from 4.27577 to 4.06673, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 16ms/step - loss: 4.0667\n",
            "Epoch 10/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 3.8506\n",
            "Epoch 00010: loss improved from 4.06673 to 3.85108, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 3.8511\n",
            "Epoch 11/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 3.6333\n",
            "Epoch 00011: loss improved from 3.85108 to 3.63332, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 3.6333\n",
            "Epoch 12/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 3.4110\n",
            "Epoch 00012: loss improved from 3.63332 to 3.41143, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 3.4114\n",
            "Epoch 13/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 3.1875\n",
            "Epoch 00013: loss improved from 3.41143 to 3.18778, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 31s 16ms/step - loss: 3.1878\n",
            "Epoch 14/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 2.9555\n",
            "Epoch 00014: loss improved from 3.18778 to 2.95570, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 2.9557\n",
            "Epoch 15/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 2.7199\n",
            "Epoch 00015: loss improved from 2.95570 to 2.71988, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 2.7199\n",
            "Epoch 16/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 2.4780\n",
            "Epoch 00016: loss improved from 2.71988 to 2.47811, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 2.4781\n",
            "Epoch 17/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 2.2429\n",
            "Epoch 00017: loss improved from 2.47811 to 2.24296, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 2.2430\n",
            "Epoch 18/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 2.0129\n",
            "Epoch 00018: loss improved from 2.24296 to 2.01298, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 2.0130\n",
            "Epoch 19/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 1.7993\n",
            "Epoch 00019: loss improved from 2.01298 to 1.79955, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 1.7996\n",
            "Epoch 20/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 1.5994\n",
            "Epoch 00020: loss improved from 1.79955 to 1.59942, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 1.5994\n",
            "Epoch 21/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 1.4283\n",
            "Epoch 00021: loss improved from 1.59942 to 1.42826, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 1.4283\n",
            "Epoch 22/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 1.2843\n",
            "Epoch 00022: loss improved from 1.42826 to 1.28448, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 1.2845\n",
            "Epoch 23/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 1.1595\n",
            "Epoch 00023: loss improved from 1.28448 to 1.15952, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 1.1595\n",
            "Epoch 24/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 1.0600\n",
            "Epoch 00024: loss improved from 1.15952 to 1.06027, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 1.0603\n",
            "Epoch 25/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.9806\n",
            "Epoch 00025: loss improved from 1.06027 to 0.98060, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.9806\n",
            "Epoch 26/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.9150\n",
            "Epoch 00026: loss improved from 0.98060 to 0.91496, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.9150\n",
            "Epoch 27/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.8547\n",
            "Epoch 00027: loss improved from 0.91496 to 0.85472, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.8547\n",
            "Epoch 28/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.8139\n",
            "Epoch 00028: loss improved from 0.85472 to 0.81397, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.8140\n",
            "Epoch 29/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.7809\n",
            "Epoch 00029: loss improved from 0.81397 to 0.78092, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.7809\n",
            "Epoch 30/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.7469\n",
            "Epoch 00030: loss improved from 0.78092 to 0.74707, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.7471\n",
            "Epoch 31/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.7190\n",
            "Epoch 00031: loss improved from 0.74707 to 0.71915, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.7192\n",
            "Epoch 32/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.6988\n",
            "Epoch 00032: loss improved from 0.71915 to 0.69869, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6987\n",
            "Epoch 33/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.6754\n",
            "Epoch 00033: loss improved from 0.69869 to 0.67535, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6753\n",
            "Epoch 34/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.6613\n",
            "Epoch 00034: loss improved from 0.67535 to 0.66122, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6612\n",
            "Epoch 35/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.6427\n",
            "Epoch 00035: loss improved from 0.66122 to 0.64273, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6427\n",
            "Epoch 36/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.6302\n",
            "Epoch 00036: loss improved from 0.64273 to 0.63025, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6303\n",
            "Epoch 37/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.6151\n",
            "Epoch 00037: loss improved from 0.63025 to 0.61518, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6152\n",
            "Epoch 38/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.6036\n",
            "Epoch 00038: loss improved from 0.61518 to 0.60355, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.6036\n",
            "Epoch 39/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.5906\n",
            "Epoch 00039: loss improved from 0.60355 to 0.59062, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5906\n",
            "Epoch 40/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.5864\n",
            "Epoch 00040: loss improved from 0.59062 to 0.58641, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5864\n",
            "Epoch 41/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.5723\n",
            "Epoch 00041: loss improved from 0.58641 to 0.57241, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5724\n",
            "Epoch 42/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.5631\n",
            "Epoch 00042: loss improved from 0.57241 to 0.56297, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5630\n",
            "Epoch 43/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.5538\n",
            "Epoch 00043: loss improved from 0.56297 to 0.55374, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5537\n",
            "Epoch 44/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.5477\n",
            "Epoch 00044: loss improved from 0.55374 to 0.54773, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5477\n",
            "Epoch 45/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.5397\n",
            "Epoch 00045: loss improved from 0.54773 to 0.53974, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5397\n",
            "Epoch 46/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.5328\n",
            "Epoch 00046: loss improved from 0.53974 to 0.53283, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5328\n",
            "Epoch 47/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.5268\n",
            "Epoch 00047: loss improved from 0.53283 to 0.52681, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5268\n",
            "Epoch 48/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.5199\n",
            "Epoch 00048: loss improved from 0.52681 to 0.51989, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5199\n",
            "Epoch 49/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.5155\n",
            "Epoch 00049: loss improved from 0.51989 to 0.51549, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5155\n",
            "Epoch 50/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.5110\n",
            "Epoch 00050: loss improved from 0.51549 to 0.51105, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5110\n",
            "Epoch 51/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.5039\n",
            "Epoch 00051: loss improved from 0.51105 to 0.50394, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5039\n",
            "Epoch 52/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.5003\n",
            "Epoch 00052: loss improved from 0.50394 to 0.50038, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.5004\n",
            "Epoch 53/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.4934\n",
            "Epoch 00053: loss improved from 0.50038 to 0.49334, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4933\n",
            "Epoch 54/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.4895\n",
            "Epoch 00054: loss improved from 0.49334 to 0.48957, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4896\n",
            "Epoch 55/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.4885\n",
            "Epoch 00055: loss improved from 0.48957 to 0.48853, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4885\n",
            "Epoch 56/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.4816\n",
            "Epoch 00056: loss improved from 0.48853 to 0.48168, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4817\n",
            "Epoch 57/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.4773\n",
            "Epoch 00057: loss improved from 0.48168 to 0.47728, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4773\n",
            "Epoch 58/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.4757\n",
            "Epoch 00058: loss improved from 0.47728 to 0.47570, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4757\n",
            "Epoch 59/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.4677\n",
            "Epoch 00059: loss improved from 0.47570 to 0.46761, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4676\n",
            "Epoch 60/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.4673\n",
            "Epoch 00060: loss improved from 0.46761 to 0.46731, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4673\n",
            "Epoch 61/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.4633\n",
            "Epoch 00061: loss improved from 0.46731 to 0.46334, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4633\n",
            "Epoch 62/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.4598\n",
            "Epoch 00062: loss improved from 0.46334 to 0.45983, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4598\n",
            "Epoch 63/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.4583\n",
            "Epoch 00063: loss improved from 0.45983 to 0.45835, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4583\n",
            "Epoch 64/70\n",
            "1952/1955 [============================>.] - ETA: 0s - loss: 0.4541\n",
            "Epoch 00064: loss improved from 0.45835 to 0.45424, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4542\n",
            "Epoch 65/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.4514\n",
            "Epoch 00065: loss improved from 0.45424 to 0.45140, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4514\n",
            "Epoch 66/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.4482\n",
            "Epoch 00066: loss improved from 0.45140 to 0.44813, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4481\n",
            "Epoch 67/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.4477\n",
            "Epoch 00067: loss improved from 0.44813 to 0.44772, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4477\n",
            "Epoch 68/70\n",
            "1954/1955 [============================>.] - ETA: 0s - loss: 0.4428\n",
            "Epoch 00068: loss improved from 0.44772 to 0.44281, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4428\n",
            "Epoch 69/70\n",
            "1953/1955 [============================>.] - ETA: 0s - loss: 0.4410\n",
            "Epoch 00069: loss improved from 0.44281 to 0.44100, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4410\n",
            "Epoch 70/70\n",
            "1955/1955 [==============================] - ETA: 0s - loss: 0.4396\n",
            "Epoch 00070: loss improved from 0.44100 to 0.43961, saving model to next_words.h5\n",
            "1955/1955 [==============================] - 30s 15ms/step - loss: 0.4396\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faca6425590>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umgxow0deGBZ"
      },
      "source": [
        "**Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2kxnik2cdKGn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  \n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "  \n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OL0bx0KentB",
        "outputId": "7ccaf825-e376-45d7-9c4c-10ceb22edacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your line: If you are not\n",
            "['you', 'are', 'not']\n",
            "entitled\n",
            "Enter your line: fixed in the\n",
            "['fixed', 'in', 'the']\n",
            "minds\n",
            "Enter your line: Monday in a\n",
            "['Monday', 'in', 'a']\n",
            "chaise\n",
            "Enter your line: take possession before\n",
            "['take', 'possession', 'before']\n",
            "michaelmas\n",
            "Enter your line: What is his\n",
            "['What', 'is', 'his']\n",
            "name\n",
            "Enter your line: 0\n",
            "Execution completed.....\n"
          ]
        }
      ],
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "  \n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "  \n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "        \n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "          \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDkM1rvPlo0x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "next word prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
